require 'net/http'
require 'net/https'
require 'uri'
require 'rss/maker'
require File.join(File.dirname(__FILE__), '..', 'lib', 'confighandler')

require 'rubygems'
require 'net/ssh'
require 'net/sftp'

# {{{ Backup Server class definition
class BackupServer
  attr_reader :hostname, :username, :storage_dir

  def initialize(hostname, username, storage_dir) 
    @hostname = hostname
    @username = username
    @storage_dir = storage_dir
  end

  # Store files as [name, Net::SFTP::Protocol::V01::Attributes] pairs.
  def all_files
    @all_files ||= load_files
  end

  private
    def load_files
      _files = []
      Net::SFTP.start(hostname, username, :auth_methods => ['publickey']) do |sftp|
        sftp.dir.foreach(storage_dir) do |f|
          _files << [f.name, f.attributes]
        end
      end
      return _files
    end
end
# }}}

BACKUP_SERVERS = {}

def get_jobs_on_node conf
  confs_dir = nil
  confs = []

  Net::SSH.start(conf['hostname'], conf['username'], :auth_methods => ['publickey']) do |ssh|
    local_hostname = ssh.exec!("hostname").chomp
    # get backup dir
    backup_dir = ssh.exec!("cat /etc/backup-toolkit.conf | grep conf | awk '{ print $2 }'").chomp
    config_filenames = ssh.exec!("ls #{backup_dir}").split()
    for c in config_filenames
      config_dump = ssh.exec!("cat #{backup_dir}/#{c}")        
      config_dump << "\n  local_hostname: #{ local_hostname }"
      config_dump << "\n  config_filename: #{ c }"
      conf = ConfigHandler::load_yaml( config_dump )       
      # {}.shift
      # {"a"=>{"b"=>1, "c"=>2}} becomes ["a", {"b"=>1, "c"=>2}]
      type, conf = conf.shift 
      conf['type'] = type
      confs << conf
    end
  end

  return confs
end

##
# generate intended filename on backup. We have to be able to duplicate the 
# naming scheme to trace all backups generated by a given backup task on a given
# node.
#
# file gets sent to: [backup_destination]/[HOSTNAME]-[local_filename]
#
# where: HOSTNAME = node.hostname, local_filename = filename generated by *-dump.sh,
#        backup_destination = node_config.backup_destination
#
def generate_intended_filename_regex conf
  prefix = "#{ conf['local_hostname'] }-\\d{4}_\\d{2}_\\d{2}"
  case conf['type']
  when /dir/
    dir_part = conf['path'].sub(/\//,'').gsub(/\//,"-")
    return "#{ prefix }-#{ dir_part }\\.tar\\.gz"
  when /mys/
    db_part = conf['database']
    return "#{ prefix }-#{ db_part }\\.sql\\.gz"
  end
end

def find_files_on_backup conf
  file_matcher = generate_intended_filename_regex conf
  unless BACKUP_SERVERS[conf['backup_hostname']]
    BACKUP_SERVERS[conf['backup_hostname']] = 
      BackupServer.new(conf['backup_hostname'], 
                       conf['backup_username'], 
                       conf['backup_destination'])
  end
  bs = BACKUP_SERVERS[conf['backup_hostname']]
  return bs.all_files.select {|file| /#{ file_matcher }/ =~ file[0] }
end

def generate_feed_for_node node
  feeds = []
  jobs = get_jobs_on_node(node)
  jobs.each do |job|
    content = RSS::Maker.make("2.0") do |m|
      m.channel.title = "backup-toolkit audit feed for node: #{ node['username'] }@#{ node['hostname'] }, job: #{ job['config_filename'] }"
      m.channel.link = "http://slsdev.net"
      m.channel.description = "node id: #{ node['id'] }; username: #{ node['username'] }; "\
                              "hostname: #{ node['hostname'] }; job name: #{ job['config_filename'] }; "\
                              "backing up to: #{ job['backup_username'] }@#{ job['backup_hostname'] }:"\
                              "~/#{ job['backup_destination'] }"
      m.items.do_sort = true
      files = find_files_on_backup job
      for f in files
        i = m.items.new_item
        i.title = f[0]
        i.link = "#"
        i.description = "#{ f[0] }: #{ f[1].size } bytes"
        i.date = Time.at(f[1].mtime)
        i.guid.content = "#{ node['username'] }-#{ node['hostname'] }-#{ job['local_hostname'] }-#{ job['config_filename'] }-#{ f[0] }"
        i.guid.isPermaLink = false
      end
    end
    FileUtils.mkdir_p 'feeds'
    feed_filename = "#{ node['username'] }-#{ node['hostname'] }-#{ job['local_hostname'] }-#{ job['config_filename'] }.xml"
    feeds << [feed_filename, content.to_s]
  end
  return feeds
end

upload_key = "Lz33aqjKwR1vlObbnm7T4zlHycOGvWHxiELkS6V65TxvTo0JqB9fvJ1GfYs5sY8kYAJlSsZ8frjkNmLtoz2"\
             "roRGgywbL24WMbOkORdUk8nrhtFjBwF1MK8Y5BdzzL1U3oCsZajhky4lQBCihK4CifoVxPhbzf6WPY80LPO"\
             "e62t4TxKI8OiwkyJJhKD6VUopEjhlNvy1kQ0GSi4pSEpXUFLFSGiQXeEUMuX8iJQPr1ptKKJVIdpXAdZpcagSgOuoN"
url = URI.parse 'https://www.slsdev.net/backup-status/upload-feed.php'

ConfigHandler::all_nodes.each do |config|
  feeds = generate_feed_for_node config
  for feed in feeds
    # POST to windev2
    filename = feed[0]
    content = feed[1]
    req = Net::HTTP::Post.new(url.path)
    puts "POSTING #{ filename } to #{ url.path }"
    req.set_form_data({ 'filename' => filename, 'content' => content, 'upload_key' => upload_key })
    _http = Net::HTTP.new(url.host, url.port)
    _http.use_ssl = true
    res = _http.start do |http| 
      http.use_ssl = true
      http.request(req)
    end
    puts res.body
  end
end

