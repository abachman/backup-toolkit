# Should be deployed with:
#   config/connections-repo.yml
#   lib/confighandler.rb
#

ACCESS_KEY = "wcl2SXc573It3BAnvW7jOMqbGm7gAF29eaC5VbphGhPsTfE0dqnsOz2ojXbt3ap8VmfviecZCT5jkM2aakZK9C2pNuoIXKtL5TDcfOheWVaw3X4Q3YfZWikhmHQ8Wh42bvWuOOxofTiNaMQLufYjle4tXHZo8n0PJvFgdUbYm6DxpYFHW9wcHT77q0UcnHKlXeM4x6a5Dy7UTT9hGGwrdpvrCKpFVhAf6ZSBecY2tu4aawtmfdDqvxhlKWzphi6"

require 'net/http'
require 'net/https'
require 'uri'
require 'rss/maker'
require File.join(File.dirname(__FILE__), '..', 'lib', 'confighandler')

require 'rubygems'
require 'net/ssh'
require 'net/sftp'

# {{{ Backup Server class definition
class BackupServer
  attr_reader :hostname, :username, :storage_dir

  def initialize(hostname, username, storage_dir) 
    @hostname = hostname
    @username = username
    @storage_dir = storage_dir
  end

  # Store files as [name, Net::SFTP::Protocol::V01::Attributes] pairs.
  def all_files
    @all_files ||= load_files
  end

  private
    def load_files
      _files = []
      Net::SFTP.start(hostname, username, :auth_methods => ['publickey']) do |sftp|
        sftp.dir.foreach(storage_dir) do |f|
          _files << { :name => f.name, :attributes => f.attributes }
        end
      end
      return _files.sort {|a, b| a[:attributes].mtime <=> b[:attributes].mtime}
    end
end
# }}}

BACKUP_SERVERS = {}

def get_jobs_on_node conf
  confs_dir = nil
  confs = []

  Net::SSH.start(conf['hostname'], conf['username'], :auth_methods => ['publickey']) do |ssh|
    local_hostname = ssh.exec!("hostname").chomp
    # get backup dir
    remote_master_config = 
      ConfigHandler::load_yaml(ssh.exec!("cat #{ conf['install_directory'] }/backup-toolkit.conf").chomp)
    jobs_directory = remote_master_config['jobs_directory']
    job_filenames = ssh.exec!("ls #{jobs_directory}").split()
    for c in job_filenames
      job_dump = ssh.exec!("cat #{jobs_directory}/#{c}")        
      job_dump << "\n  local_hostname: #{ local_hostname }"
      job_dump << "\n  job_filename: #{ c }"
      conf = ConfigHandler::load_yaml( job_dump )       
      # {}.shift
      # {"a"=>{"b"=>1, "c"=>2}} becomes ["a", {"b"=>1, "c"=>2}]
      type, conf = conf.shift 
      conf['type'] = type
      confs << conf
    end
  end

  return confs
end

##
# generate intended filename on backup. We have to be able to duplicate the 
# naming scheme to trace all backups generated by a given backup task on a given
# node.
#
# file gets sent to: [backup_destination]/[HOSTNAME]-[local_filename]
#
# where: HOSTNAME = node.hostname, local_filename = filename generated by *-dump.sh,
#        backup_destination = node_config.backup_destination
#
def generate_intended_filename_regex conf
  prefix = "#{ conf['local_hostname'] }-\\d{4}_\\d{2}_\\d{2}"
  case conf['type']
  when /dir/
    dir_part = conf['path'].sub(/\//,'').gsub(/\//,"-")
    return "#{ prefix }-#{ dir_part }\\.tar\\.gz"
  when /mys/
    db_part = conf['database']
    return "#{ prefix }-#{ db_part }\\.sql\\.gz"
  end
end

# Get 20 most recent files
def find_files_on_backup conf
  file_matcher = generate_intended_filename_regex conf
  unless BACKUP_SERVERS[conf['backup_hostname']]
    bs = BackupServer.new(conf['backup_hostname'], 
                          conf['backup_username'], 
                          conf['backup_destination'])
    BACKUP_SERVERS[conf['backup_hostname']] = bs
  end
  bs = BACKUP_SERVERS[conf['backup_hostname']]
  files = bs.all_files.select {|file| /#{ file_matcher }/ =~ file[:name] }
  return files.reverse()[0,20]
end

def generate_feeds_for_node node
  feeds = []
  FileUtils.mkdir_p 'feeds'

  jobs = get_jobs_on_node(node)
  jobs.each do |job|
    files = find_files_on_backup job
    content = RSS::Maker.make("2.0") do |m|
      m.channel.title = "backup-toolkit audit feed for node: #{ node['username'] }@#{ node['hostname'] }, job: #{ job['job_filename'] }"
      m.channel.link = "http://slsdev.net"
      m.channel.description = "node id: #{ node['id'] }; username: #{ node['username'] }; "\
                              "hostname: #{ node['hostname'] }; job name: #{ job['job_filename'] }; "\
                              "backing up to: #{ job['backup_username'] }@#{ job['backup_hostname'] }:"\
                              "~/#{ job['backup_destination'] }"
      m.items.do_sort = true
      for f in files
        i = m.items.new_item
        i.title = f[:name]
        i.link = "#"
        i.description = "#{ f[:name] }: #{ f[:attributes].size } bytes"
        i.date = Time.at(f[:attributes].mtime)
        i.guid.content = "#{ node['username'] }-#{ node['hostname'] }-"\
                         "#{ job['local_hostname'] }-#{ job['job_filename'] }-"\
                         "#{ f[:name] }"
        i.guid.isPermaLink = false
      end
    end
    feed_filename = "#{ node['username'] }-#{ node['hostname'] }-#{ job['local_hostname'] }-#{ job['job_filename'] }.xml"
    feeds << { 
      :filename => feed_filename, 
      :content => content.to_s,
      :job => job, 
      :files => files 
    }
  end
  return feeds
end

def run()
  one_day_ago = Time.now - (60 * 60 * 24)
  snapshot = RSS::Maker.make("2.0") do |m|
    m.channel.title = "backup-toolkit daily activity"
    m.channel.link = "http://slsdev.net"
    m.channel.description = "backup-toolkit daily activity snapshot, current status of all packages."
    m.items.do_sort = true

    i = m.items.new_item 
    i.description = "<p><strong>Backup Toolkit Snapshot</strong></p><ul>"
    i.date = Time.now
    i.guid.content = "admin-snapshot-#{ Time.now }"
    i.guid.isPermaLink = false

    visited_nodes = {}
    ConfigHandler::all_nodes.each do |node|
      next if visited_nodes["#{ node['username'] }@#{ node['hostname'] }"]
      visited_nodes["#{ node['username'] }@#{ node['hostname'] }"] = true

      feeds = generate_feeds_for_node node
      puts "using #{ node.inspect }"
      for feed in feeds
        # Add this feed's info to snapshot
        job_title = "#{feed[:job]['job_filename']} on #{feed[:job]['local_hostname']}"
        if Time.at(feed[:files].first()[:attributes].mtime) > one_day_ago 
          status_message = "up to date."
        else
          status_message = "<b><span style='background-color:red;'>IS NOT UP TO DATE!!!!</span></b>"
        end
        i.description << <<-EOS
          <li>
            <p><em>#{ job_title }</em></p>
            <p><a href="https://slsdev.net/backup-status/get-feed.php?filename=#{ feed[:filename] }&access_key=#{ACCESS_KEY}">
              #{ feed[:filename] }</a></p>
            <p>config: <code>#{ feed[:job].inspect }</code></p>
            <p>The current file is
              #{ status_message } 
            </p>
          </li>
        EOS
        
        # POST to windev2
        filename = feed[:filename]
        content = feed[:content]
        post_to_windev2 filename, content.to_s
      end
    end
    i.description << "</ul>"
  end # finish today's admin rss feed
  post_to_windev2 "Backup Toolkit Administrative Feed", snapshot.to_s
end

def post_to_windev2 filename, content
  url = URI.parse 'https://www.slsdev.net/backup-status/upload-feed.php'
  req = Net::HTTP::Post.new(url.path)
  print "POSTING #{ filename } to #{ url.path }: "
  req.set_form_data({ 'filename' => filename, 'content' => content, 'upload_key' => ACCESS_KEY })
  _http = Net::HTTP.new(url.host, url.port)
  _http.verify_mode = OpenSSL::SSL::VERIFY_NONE
  _http.use_ssl = true
  res = _http.start do |http| 
    http.use_ssl = true
    http.request(req)
  end 
  puts res.body
end

run()
